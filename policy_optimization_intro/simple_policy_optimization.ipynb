{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucas.gandara/Documents/python_scripts/RL_OpenAI/env/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v0'\n",
    "render = True\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(sizes, activation=tf.keras.activations.tanh, output_activation=tf.identity):\n",
    "    # Build a feedforward neural network.\n",
    "    layers = []\n",
    "    layers.append(tf.keras.layers.Input(sizes[0]))\n",
    "    for size in sizes[1:-1]:\n",
    "        layers.append(tf.keras.layers.Dense(units=size, activation=activation))\n",
    "    layers.append(tf.keras.layers.Dense(units=sizes[-1], activation=output_activation))\n",
    "    return tf.keras.Sequential(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make function to compute action distribution\n",
    "def get_policy(obs, logits_net):\n",
    "    logits = logits_net(obs)\n",
    "    return tfp.distributions.Categorical(logits=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make action selection function (outputs int actions, sampled from policy)\n",
    "def get_action(obs, logits_net):\n",
    "    return get_policy(obs, logits_net).sample().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make loss function whose gradient, for the right data, is policy gradient\n",
    "def compute_loss(obs, act, weights, logits_net):\n",
    "    logp = get_policy(obs, logits_net).log_prob(act)\n",
    "    return -(logp * weights).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(env, batch_size, optimizer, logits_net):\n",
    "    # make some empty lists for logging.\n",
    "    batch_obs = []          # for observations\n",
    "    batch_acts = []         # for actions\n",
    "    batch_weights = []      # for R(tau) weighting in policy gradient\n",
    "    batch_rets = []         # for measuring episode returns\n",
    "    batch_lens = []         # for measuring episode lengths\n",
    "\n",
    "    # reset episode-specific variables\n",
    "    obs = env.reset() # first obs comes from starting distribution\n",
    "    donde = False     # signal from environment that episode is over  \n",
    "    ep_rews = []      # list for rewards accrued throughout ep\n",
    "\n",
    "    # render first episode of each epoch\n",
    "    finished_rendering_this_epoch = False\n",
    "\n",
    "    # collect experience by acting in the environment with current policy\n",
    "    while True:\n",
    "        if (not finished_rendering_this_epoch):\n",
    "            env.render()\n",
    "\n",
    "        # Save the observation\n",
    "        batch_obs.append(batch_obs.copy())\n",
    "\n",
    "        # act in the environment\n",
    "        act = get_action(tf.constant(obs[0], dtype=tf.dtypes.float32), logits_net)\n",
    "        obs, rew, done, _ = env.step(act)\n",
    "\n",
    "        # save action, reward\n",
    "        batch_acts.append(act)\n",
    "        ep_rews.append(rew)\n",
    "\n",
    "        if done:\n",
    "            # if episode is over, record info about episode\n",
    "            ep_ret, ep_len = sum(ep_rews), len(ep_rews)\n",
    "            batch_rets.append(ep_ret)\n",
    "            batch_lens.append(ep_len)\n",
    "\n",
    "            # the weight for each logprob(a|s) is R(tau)\n",
    "            batch_weights += [ep_ret] * ep_len\n",
    "\n",
    "            # Reset episode-specific variables\n",
    "            obs, done, ep_rews = env.reset(), False, []\n",
    "\n",
    "            # won't render again this epoch\n",
    "            finished_rendering_this_epoch = True\n",
    "\n",
    "            # end experience loop if we have enough of it\n",
    "            if len(batch_obs) > batch_size:\n",
    "                break\n",
    "\n",
    "    # Reset the optimizer\n",
    "    for var in optimizer.variables():\n",
    "        var.assign(tf.zeros_like(var))\n",
    "\n",
    "    batch_loss = compute_loss(\n",
    "        batch_obs = tf.constant(batch_obs, dtype=tf.dtypes.float32),\n",
    "        batch_acts = tf.constant(batch_acts, dtype=tf.dtypes.float32),\n",
    "        batch_weights = tf.constant(batch_weights, dtype=tf.dtypes.float32),\n",
    "        logits_net=logits_net\n",
    "    )\n",
    "\n",
    "    batch_loss.backward()\n",
    "    optimizer.step()\n",
    "    return batch_loss, batch_rets, batch_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env_name='CartPole-v0', hidden_sizes=[32], lr=1e-2, epochs=50, batch_size=5000, render=False):\n",
    "    env = gym.make(env_name)\n",
    "\n",
    "    assert isinstance(env.observation_space, gym.spaces.Box), \\\n",
    "        \"This example only works for envs with continuous state spaces.\"\n",
    "    assert isinstance(env.action_space, gym.spaces.Discrete), \\\n",
    "        \"This example only works for envs with discrete action spaces.\"\n",
    "\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    print(f'Observation dimention: {obs_dim}')\n",
    "    n_acts = env.action_space.n\n",
    "    print(f'Number of possible actions: {n_acts}')\n",
    "\n",
    "    # make core of policy network\n",
    "    logits_net = mlp(sizes=[obs_dim]+hidden_sizes+[n_acts])\n",
    "\n",
    "    # make optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam(lr)\n",
    "\n",
    "    # Train loop\n",
    "    for i in range(epochs):\n",
    "        batch_loss, batch_rets, batch_lens = train_epoch(env, batch_size, optimizer, logits_net)\n",
    "        print('epoch: %3d \\t loss: %.3f \\t return: %.3f \\t ep_len: %.3f'%\n",
    "            (i, batch_loss, np.mean(batch_rets), np.mean(batch_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation dimention: 4\n",
      "Number of possible actions: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucas.gandara/Documents/python_scripts/RL_OpenAI/env/lib/python3.9/site-packages/gym/envs/classic_control/cartpole.py:211: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"CartPole-v0\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer 'sequential_10' (type Sequential).\n\nInput 0 of layer \"dense_29\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (4,)\n\nCall arguments received by layer 'sequential_10' (type Sequential):\n  • inputs=tf.Tensor(shape=(4,), dtype=float32)\n  • training=None\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/lucas.gandara/Documents/python_scripts/RL_OpenAI/policy_optimization_intro/simple_policy_optimization.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lucas.gandara/Documents/python_scripts/RL_OpenAI/policy_optimization_intro/simple_policy_optimization.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train(env_name\u001b[39m=\u001b[39;49menv_name, render\u001b[39m=\u001b[39;49mrender, lr\u001b[39m=\u001b[39;49mlr)\n",
      "\u001b[1;32m/Users/lucas.gandara/Documents/python_scripts/RL_OpenAI/policy_optimization_intro/simple_policy_optimization.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lucas.gandara/Documents/python_scripts/RL_OpenAI/policy_optimization_intro/simple_policy_optimization.ipynb#X11sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Train loop\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lucas.gandara/Documents/python_scripts/RL_OpenAI/policy_optimization_intro/simple_policy_optimization.ipynb#X11sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lucas.gandara/Documents/python_scripts/RL_OpenAI/policy_optimization_intro/simple_policy_optimization.ipynb#X11sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     batch_loss, batch_rets, batch_lens \u001b[39m=\u001b[39m train_epoch(env, batch_size, optimizer, logits_net)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lucas.gandara/Documents/python_scripts/RL_OpenAI/policy_optimization_intro/simple_policy_optimization.ipynb#X11sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mepoch: \u001b[39m\u001b[39m%3d\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m loss: \u001b[39m\u001b[39m%.3f\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m return: \u001b[39m\u001b[39m%.3f\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m ep_len: \u001b[39m\u001b[39m%.3f\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m%\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lucas.gandara/Documents/python_scripts/RL_OpenAI/policy_optimization_intro/simple_policy_optimization.ipynb#X11sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m         (i, batch_loss, np\u001b[39m.\u001b[39mmean(batch_rets), np\u001b[39m.\u001b[39mmean(batch_lens)))\n",
      "\u001b[1;32m/Users/lucas.gandara/Documents/python_scripts/RL_OpenAI/policy_optimization_intro/simple_policy_optimization.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lucas.gandara/Documents/python_scripts/RL_OpenAI/policy_optimization_intro/simple_policy_optimization.ipynb#X11sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m batch_obs\u001b[39m.\u001b[39mappend(batch_obs\u001b[39m.\u001b[39mcopy())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lucas.gandara/Documents/python_scripts/RL_OpenAI/policy_optimization_intro/simple_policy_optimization.ipynb#X11sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# act in the environment\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lucas.gandara/Documents/python_scripts/RL_OpenAI/policy_optimization_intro/simple_policy_optimization.ipynb#X11sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m act \u001b[39m=\u001b[39m get_action(tf\u001b[39m.\u001b[39;49mconstant(obs[\u001b[39m0\u001b[39;49m], dtype\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mdtypes\u001b[39m.\u001b[39;49mfloat32), logits_net)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lucas.gandara/Documents/python_scripts/RL_OpenAI/policy_optimization_intro/simple_policy_optimization.ipynb#X11sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m obs, rew, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(act)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lucas.gandara/Documents/python_scripts/RL_OpenAI/policy_optimization_intro/simple_policy_optimization.ipynb#X11sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# save action, reward\u001b[39;00m\n",
      "\u001b[1;32m/Users/lucas.gandara/Documents/python_scripts/RL_OpenAI/policy_optimization_intro/simple_policy_optimization.ipynb Cell 9\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lucas.gandara/Documents/python_scripts/RL_OpenAI/policy_optimization_intro/simple_policy_optimization.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_action\u001b[39m(obs, logits_net):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lucas.gandara/Documents/python_scripts/RL_OpenAI/policy_optimization_intro/simple_policy_optimization.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m get_policy(obs, logits_net)\u001b[39m.\u001b[39msample()\u001b[39m.\u001b[39mitem()\n",
      "\u001b[1;32m/Users/lucas.gandara/Documents/python_scripts/RL_OpenAI/policy_optimization_intro/simple_policy_optimization.ipynb Cell 9\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lucas.gandara/Documents/python_scripts/RL_OpenAI/policy_optimization_intro/simple_policy_optimization.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_policy\u001b[39m(obs, logits_net):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lucas.gandara/Documents/python_scripts/RL_OpenAI/policy_optimization_intro/simple_policy_optimization.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     logits \u001b[39m=\u001b[39m logits_net(obs)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lucas.gandara/Documents/python_scripts/RL_OpenAI/policy_optimization_intro/simple_policy_optimization.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m tfp\u001b[39m.\u001b[39mdistributions\u001b[39m.\u001b[39mCategorical(logits\u001b[39m=\u001b[39mlogits)\n",
      "File \u001b[0;32m~/Documents/python_scripts/RL_OpenAI/env/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Documents/python_scripts/RL_OpenAI/env/lib/python3.9/site-packages/keras/src/engine/input_spec.py:253\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    251\u001b[0m     ndim \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape\u001b[39m.\u001b[39mrank\n\u001b[1;32m    252\u001b[0m     \u001b[39mif\u001b[39;00m ndim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m ndim \u001b[39m<\u001b[39m spec\u001b[39m.\u001b[39mmin_ndim:\n\u001b[0;32m--> 253\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    254\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00minput_index\u001b[39m}\u001b[39;00m\u001b[39m of layer \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlayer_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    255\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mis incompatible with the layer: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    256\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexpected min_ndim=\u001b[39m\u001b[39m{\u001b[39;00mspec\u001b[39m.\u001b[39mmin_ndim\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    257\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfound ndim=\u001b[39m\u001b[39m{\u001b[39;00mndim\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    258\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFull shape received: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(shape)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    259\u001b[0m         )\n\u001b[1;32m    260\u001b[0m \u001b[39m# Check dtype.\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[39mif\u001b[39;00m spec\u001b[39m.\u001b[39mdtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer 'sequential_10' (type Sequential).\n\nInput 0 of layer \"dense_29\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (4,)\n\nCall arguments received by layer 'sequential_10' (type Sequential):\n  • inputs=tf.Tensor(shape=(4,), dtype=float32)\n  • training=None\n  • mask=None"
     ]
    }
   ],
   "source": [
    "train(env_name=env_name, render=render, lr=lr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
